# Dynamic Model Switching Guide

**Reading time**: 15 minutes | **Skill level**: Intermediate | **Version**: v1.4.0 | **Last updated**: 2026-01-22

---

Le script `claude-switch` supporte maintenant le changement dynamique de mod√®le pour GitHub Copilot via la variable d'environnement `COPILOT_MODEL` et pour Ollama via `OLLAMA_MODEL`.

## Mod√®les disponibles via copilot-api

‚ö†Ô∏è **Important**: Tous les mod√®les ne sont pas compatibles avec l'endpoint `/chat/completions` utilis√© par copilot-api. Voir section Compatibilit√© ci-dessous.

### ‚úÖ Claude Models (Test√©s et fonctionnels)
- `claude-sonnet-4.5` ‚≠ê (d√©faut, meilleur rapport qualit√©/vitesse)
- `claude-opus-4.5` (meilleure qualit√©, plus lent)
- `claude-haiku-4.5` (le plus rapide)
- `claude-sonnet-4`

### ‚úÖ GPT Models (Compatibles avec `/chat/completions`)
- ‚úÖ `gpt-4.1` (d√©faut, 0x premium, √©quilibr√©) ‚≠ê
- ‚úÖ `gpt-5` (raisonnement avanc√©, 1x premium)
- ‚úÖ `gpt-5-mini` (ultra rapide, 0x premium)
- ‚ö†Ô∏è `gpt-4o` (stable mais **d√©pr√©ci√© le 17 f√©vrier 2026**)

### ‚ùå GPT Codex Models (INCOMPATIBLES - requi√®rent endpoint `/responses`)
- ‚ùå `gpt-5.2-codex` (GA depuis 14 jan 2026, mais `/responses` uniquement)
- ‚ùå `gpt-5.1-codex` (Preview, `/responses` uniquement)
- ‚ùå `gpt-5.1-codex-mini` (Preview, `/responses` uniquement)
- ‚ùå `gpt-5-codex` (Preview, `/responses` uniquement)

### ‚úÖ Gemini Models
- `gemini-2.5-pro` (stable, recommand√©)
- `gemini-3-pro-preview` (preview, peut retourner 400)
- `gemini-3-flash-preview` (preview, rapide mais instable)

### ‚ö†Ô∏è Autres
- ‚úÖ `grok-code-fast-1` (rapide, sp√©cialis√© code)
- ‚úÖ `raptor-mini` (l√©ger, rapide)

### ‚ùå Mod√®les d√©pr√©ci√©s (17 f√©vrier 2026)
- `claude-opus-4.1` ‚Üí Utiliser `claude-opus-4.5` √† la place

## ‚ö†Ô∏è Compatibilit√© des mod√®les - Limitation Architecturale Majeure

### Probl√®me: Endpoint `/responses` vs `/chat/completions`

**copilot-api (v0.7.0) ne supporte QUE l'endpoint `/chat/completions`**. Tous les mod√®les GPT Codex (sp√©cialis√©s code) n√©cessitent le nouvel endpoint `/responses` lanc√© par OpenAI en octobre 2025.

### Incompatibilit√© Confirm√©e - Tous les Mod√®les Codex

**TOUS les mod√®les de la famille Codex sont INCOMPATIBLES** :

| Mod√®le | Status OpenAI | Erreur copilot-api |
|--------|---------------|-------------------|
| `gpt-5.2-codex` | GA (14 jan 2026) | ‚ùå "not accessible via /chat/completions endpoint" |
| `gpt-5.1-codex` | Preview | ‚ùå M√™me erreur |
| `gpt-5.1-codex-mini` | Preview | ‚ùå M√™me erreur |
| `gpt-5-codex` | Preview | ‚ùå M√™me erreur |

**Cause technique** : Les mod√®les Codex utilisent un paradigme stateful avec `previous_response_id` pour le contexte, incompatible avec l'API Chat Completions classique.

### Solutions de Contournement

**Option 1 : Utiliser les mod√®les GPT compatibles** (recommand√©) :
- `gpt-4.1` (0x premium, inclus, √©quilibr√©)
- `gpt-5` (1x premium, raisonnement avanc√©)
- `gpt-5-mini` (0x premium, rapide)

**Option 2 : Utiliser Claude via Copilot** (100% compatible) :
- `claude-sonnet-4.5` (meilleur rapport qualit√©/vitesse)
- `claude-opus-4.5` (qualit√© maximale)

**Option 3 : Attendre une mise √† jour copilot-api** :
- Requiert r√©√©criture du routeur API (modification majeure)
- Aucune roadmap annonc√©e √† ce jour
- PR communautaire en discussion mais pas de timeline

**Option 4 : Utiliser les interfaces natives** :
- VS Code native (Codex disponible)
- GitHub.com Chat (Codex disponible)
- copilot-cli (Codex disponible)

## M√©thodes pour changer de mod√®le

### M√©thode 1: Variable d'environnement (One-shot)

```bash
# Utiliser Opus pour une session
COPILOT_MODEL=claude-opus-4.5 claude-switch copilot

# Utiliser GPT-4.1 (compatible, inclus)
COPILOT_MODEL=gpt-4.1 ccc

# Utiliser Haiku (ultra rapide)
COPILOT_MODEL=claude-haiku-4.5 ccc
```

### M√©thode 2: Aliases pr√©d√©finis (Recommand√©)

D√©j√† configur√©s dans votre `~/.zshrc`:

```bash
# Apr√®s `source ~/.bash_aliases`:

ccc-opus     # Claude Opus 4.5 (meilleure qualit√©)
ccc-sonnet   # Claude Sonnet 4.5 (d√©faut)
ccc-haiku    # Claude Haiku 4.5 (ultra rapide)
ccc-gpt      # GPT-4.1 (alternative GPT, 0x premium)
```

### M√©thode 3: Export permanent (Session shell)

```bash
# D√©finir pour toute la session shell
export COPILOT_MODEL=claude-opus-4.5

# Tous les appels √† `ccc` utiliseront Opus
ccc
ccc
ccc

# Reset au d√©faut
unset COPILOT_MODEL
```

## Comparaison des mod√®les Claude via Copilot

| Mod√®le | Qualit√© | Vitesse | Usage recommand√© |
|--------|---------|---------|------------------|
| **claude-opus-4.5** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üê¢ Lent | Code critique, architecture |
| **claude-sonnet-4.5** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° Rapide | D√©veloppement quotidien (d√©faut) |
| **claude-haiku-4.5** | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° Ultra rapide | Refactoring simple, questions rapides |

## Exemples d'usage

### Workflow hybride

```bash
# Morning: Exploration rapide avec Haiku
ccc-haiku
> Explore this codebase structure

# Afternoon: D√©veloppement avec Sonnet (√©quilibre)
ccc-sonnet
> Implement user authentication

# Code Review: Qualit√© maximale avec Opus
ccc-opus
> Review this PR for security issues and architecture
```

### Comparaison de mod√®les

```bash
# Tester la m√™me question avec diff√©rents mod√®les
COPILOT_MODEL=claude-sonnet-4.5 ccc
> Optimize this algorithm

COPILOT_MODEL=gpt-4.1 ccc
> Optimize this algorithm

COPILOT_MODEL=claude-opus-4.5 ccc
> Optimize this algorithm
```

## Mod√®les GPT via Copilot

### GPT-4.1 (Recommand√©, √©quilibr√©)

```bash
COPILOT_MODEL=gpt-4.1 ccc
```

**Avantages**:
- ‚úÖ 100% compatible avec copilot-api (endpoint `/chat/completions`)
- 0x premium (inclus dans l'abonnement)
- √âquilibr√© qualit√©/vitesse
- Bon pour usage g√©n√©ral

**Limitations**:
- ‚ö†Ô∏è Validation MCP stricte (peut √©chouer sur certains outils)
- Workaround: `DISABLE_NON_ESSENTIAL_MODEL_CALLS=1`

**Usage**: Alternative GPT √† Claude Sonnet pour le d√©veloppement quotidien

### GPT-5 (Raisonnement avanc√©)

```bash
COPILOT_MODEL=gpt-5 ccc
```

**Avantages**:
- Raisonnement avanc√©
- Meilleure qualit√© que GPT-4.1

**Limitations**:
- 1x premium (co√ªt suppl√©mentaire)

**Usage**: T√¢ches complexes n√©cessitant raisonnement pouss√©

### GPT-5-mini (Ultra rapide)

```bash
COPILOT_MODEL=gpt-5-mini ccc
```

**Avantages**:
- Tr√®s rapide
- 0x premium (inclus)
- Bon pour questions simples

**Usage**: Refactoring basique, questions rapides

## Mod√®les Gemini via Copilot

### ‚ö†Ô∏è Compatibilit√© Agentic Variable

Les mod√®les Gemini ont une **compatibilit√© limit√©e avec le mode agentic** (tool calling, file creation, MCP tools) en raison de diff√©rences dans la traduction des formats tool calling Claude ‚Üí OpenAI ‚Üí Gemini.

### Gemini 2.5 Pro (Stable - Recommand√© avec r√©serves)

```bash
COPILOT_MODEL=gemini-2.5-pro ccc
# Or alias
ccc-gemini
```

**Avantages**:
- ‚úÖ Stable (non-preview)
- ‚úÖ Prompts simples fonctionnent bien
- ‚úÖ Bon rapport qualit√©/prix
- ‚ö° Rapide

**Limitations**:
- ‚ö†Ô∏è **Mode agentic limit√©** : File creation, MCP tools peuvent √©chouer
- ‚ö†Ô∏è **D√©pr√©ci√© le 17 f√©vrier 2026** ‚Üí Migration vers gemini-3-pro-preview n√©cessaire
- ‚ö†Ô∏è Complex multi-tool workflows probl√©matiques

**Usage recommand√©**:
- ‚úÖ Prompts simples (questions, explications, suggestions)
- ‚ö†Ô∏è **√âviter** : File creation, complex tool chains
- üö´ **Production** : Pr√©f√©rer Claude ou GPT-4.1

### Gemini 3 Pro Preview (Exp√©rimental)

```bash
COPILOT_MODEL=gemini-3-pro-preview ccc
```

**Avantages**:
- ‚úÖ Mod√®le le plus r√©cent
- ‚úÖ Prompts simples fonctionnent

**Limitations**:
- ‚ùå **Mode agentic TR√àS instable** : Tool calling √©choue fr√©quemment
- ‚ùå File operations silently fail
- ‚ùå Subagent spawning unreliable
- ‚ùå MCP tools inconsistent
- ‚ö†Ô∏è Mod√®le preview (instable par nature)

**Workaround requis pour agentic**:
```bash
# Router tool calls through GPT subagent
COPILOT_MODEL=gemini-3-pro-preview CLAUDE_CODE_SUBAGENT_MODEL=gpt-5-mini ccc
```

**Usage recommand√©**:
- ‚úÖ Exp√©rimentation, prototypes
- ‚úÖ Prompts simples uniquement
- üö´ **Production** : NE PAS utiliser pour code critique
- üö´ **Agentic tasks** : Utiliser Claude ou GPT-4.1

### Gemini 3 Flash Preview (Rapide mais instable)

```bash
COPILOT_MODEL=gemini-3-flash-preview ccc
```

**Avantages**:
- ‚ö° Tr√®s rapide
- üí∞ √âconomique

**Limitations**:
- ‚ùå M√™mes probl√®mes agentic que Gemini 3 Pro
- ‚ùå Moins pr√©cis que Pro
- ‚ö†Ô∏è Preview (instable)

**Usage recommand√©**:
- ‚úÖ Tests de performance
- üö´ **Production** : √âviter compl√®tement

### Tableau de Compatibilit√© Gemini

| Mod√®le | Prompts Simples | Mode Agentic | File Creation | MCP Tools | Status | Recommandation |
|--------|----------------|--------------|---------------|-----------|--------|----------------|
| `gemini-2.5-pro` | ‚úÖ Excellent | ‚ö†Ô∏è Limit√© | ‚ö†Ô∏è Instable | ‚ö†Ô∏è Partiel | Deprecating 2/17/26 | ‚ö†Ô∏è **Transition vers Claude** |
| `gemini-3-pro-preview` | ‚úÖ Bon | ‚ùå Mauvais | ‚ùå √âchoue | ‚ùå √âchoue | Experimental | ‚ùå **Requiert workaround** |
| `gemini-3-flash-preview` | ‚úÖ Bon | ‚ùå Mauvais | ‚ùå √âchoue | ‚ùå √âchoue | Experimental | üö´ **√âviter** |

**Comparaison avec alternatives stables** :

| Aspect | Gemini 2.5 Pro | Gemini 3 Preview | Claude Sonnet | GPT-4.1 |
|--------|----------------|------------------|---------------|---------|
| **Prompts simples** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Agentic mode** | ‚ö†Ô∏è Limit√© | ‚ùå Mauvais | ‚úÖ Excellent | ‚úÖ Bon |
| **File creation** | ‚ö†Ô∏è Instable | ‚ùå √âchoue | ‚úÖ Fiable | ‚úÖ Fiable |
| **MCP tools** | ‚ö†Ô∏è Partiel | ‚ùå √âchoue | ‚úÖ 100% | ‚úÖ ~80% |
| **Stabilit√©** | ‚ö†Ô∏è Moyenne | ‚ùå Faible | ‚úÖ Excellente | ‚úÖ Excellente |
| **Production ready** | ‚ö†Ô∏è Non | üö´ Non | ‚úÖ **Oui** | ‚úÖ **Oui** |

### Recommandations d'Usage Gemini

**‚úÖ Sc√©narios adapt√©s**:
```bash
# Questions simples
COPILOT_MODEL=gemini-2.5-pro ccc -p "Explain this code"

# Suggestions sans modification
COPILOT_MODEL=gemini-2.5-pro ccc -p "Suggest improvements"

# Analyses statiques
COPILOT_MODEL=gemini-2.5-pro ccc -p "Find bugs in this file"
```

**üö´ Sc√©narios √† √©viter**:
```bash
# ‚ùå File creation - Pr√©f√©rer Claude
# COPILOT_MODEL=gemini-3-pro-preview ccc -p "Create hello.txt"
ccc-sonnet -p "Create hello.txt"  # ‚úÖ Use Claude instead

# ‚ùå Multi-tool workflows - Pr√©f√©rer Claude
# COPILOT_MODEL=gemini-3-pro-preview ccc -p "Refactor this module"
ccc-sonnet -p "Refactor this module"  # ‚úÖ Use Claude instead

# ‚ùå MCP tool usage - Pr√©f√©rer Claude
# COPILOT_MODEL=gemini-2.5-pro ccc -p "Use grep to find TODOs"
ccc-sonnet -p "Use grep to find TODOs"  # ‚úÖ Use Claude instead
```

### Migration Path pour utilisateurs Gemini

**Si tu utilises actuellement gemini-2.5-pro** :

```
Aujourd'hui (avant 17 f√©v 2026):
‚îú‚îÄ Prompts simples ‚Üí Continue avec gemini-2.5-pro
‚îú‚îÄ Agentic tasks ‚Üí Migre vers ccc-sonnet (Claude)
‚îî‚îÄ Production code ‚Üí Migre vers ccc-sonnet

Apr√®s 17 f√©vrier 2026:
‚îú‚îÄ ALL scenarios ‚Üí ccc-sonnet (Claude Sonnet 4.5)
‚îú‚îÄ Alternative ‚Üí COPILOT_MODEL=gpt-4.1 ccc
‚îî‚îÄ Exp√©rimental ‚Üí gemini-3-pro-preview + subagent workaround
```

**Si tu veux tester gemini-3-pro-preview** :

```bash
# Avec subagent workaround (requis pour agentic)
COPILOT_MODEL=gemini-3-pro-preview CLAUDE_CODE_SUBAGENT_MODEL=gpt-5-mini ccc

# Mais pr√©f√©rer directement Claude pour production
ccc-sonnet  # ‚úÖ Plus stable, meilleure qualit√©
```

### Diagnostic Gemini

Si tu rencontres des probl√®mes avec Gemini en mode agentic :

```bash
# Dans le projet cc-copilot-bridge
./scripts/test-gemini.sh

# Voir le rapport de diagnostic
cat debug-gemini/diagnostic-report.md

# Analyser les logs copilot-api
./scripts/analyze-copilot-logs.sh debug-gemini/copilot-api-verbose.log
```

**Voir aussi** :
- [TROUBLESHOOTING.md - Gemini Agentic Mode Issues](TROUBLESHOOTING.md#-gemini-agentic-mode-issues-copilot-api)
- [copilot-api Issue #151](https://github.com/ericc-ch/copilot-api/issues/151) - Gemini compatibility

### Conclusion : Quand utiliser Gemini ?

**Gemini est adapt√© UNIQUEMENT pour** :
- ‚úÖ Questions et explications simples
- ‚úÖ Analyses statiques (pas de modification)
- ‚úÖ Exp√©rimentation et tests

**Pour tout le reste, pr√©f√©rer** :
- ‚≠ê **Claude Sonnet 4.5** (`ccc-sonnet`) - Meilleur choix g√©n√©ral
- ‚úÖ **GPT-4.1** (`ccc-gpt`) - Alternative solide
- ‚úÖ **Claude Opus 4.5** (`ccc-opus`) - Qualit√© maximale

## Mod√®les Ollama (Local - Updated January 2026)

### Mod√®les recommand√©s

**Important** : SWE-bench Verified mesure la performance agentic r√©elle (r√©solution de GitHub issues avec tool calling, √©dition multi-fichiers). Les scores HumanEval √©lev√©s ne garantissent PAS une bonne performance agentic.

| Model | SWE-bench Verified | Params | Practical Status | Use Case |
|-------|-------------------|--------|------------------|----------|
| **devstral-small-2** (default) | **68.0%** | 24B | ‚úÖ Best agentic | Daily coding, proven reliable |
| **qwen3-coder:30b** | **69.6%** | 30B | ‚ö†Ô∏è Needs template work | Highest bench, config issues |
| **ibm/granite4:small-h** | ~62% | 32B (9B active) | ‚úÖ Long context | 70% less VRAM, 1M context |
| **glm-4.7-flash** | ~65-68% (estimated) | 30B MoE (3B active) | ‚ùå Untested | Speed-optimized variant |

**Sources des benchmarks** :
- Devstral-small-2 : [Mistral AI](https://mistral.ai/news/devstral-2-vibe-cli) - 68.0% SWE-bench Verified
- Qwen3-coder : [Index.dev](https://www.index.dev/blog/qwen-ai-coding-review) - 69.6% SWE-bench Verified, **MAIS** [Qwen blog](https://qwenlm.github.io/blog/qwen3-coder/) indique "Agent RL post-training" (bolt-on)
- GLM-4.7 full : [Z.AI](https://z.ai/blog/glm-4.7) - 73.8% (Flash variant [WaveSpeedAI](https://wavespeed.ai/blog/posts/glm-4-7-flash-vs-glm-4-7/) = "tier lower")

**Pourquoi Devstral malgr√© SWE-bench inf√©rieur ?**

Qwen3-coder a 1.6% de SWE-bench en plus, MAIS :
- Devstral : **Architecture native** pour agentic software engineering ([Mistral AI](https://mistral.ai/news/devstral-2-vibe-cli))
- Qwen3 : **Post-training bolt-on** (Agent RL ajout√© apr√®s coup) ‚Üí "needs template work" en pratique
- Devstral : **Prouv√© fiable** avec Claude Code (CLAUDE.md testing)
- Qwen3 : **Gap bench vs r√©alit√©** (comme Llama3.1:8b : 68% HumanEval mais 15% SWE-bench)

**‚ö†Ô∏è Mod√®les NON recommand√©s** (SWE-bench faible malgr√© HumanEval √©lev√©) :
- CodeLlama:13b : 40% SWE-bench (pas de tool calling fiable)
- Llama3.1:8b : **15%** SWE-bench ("catastrophic failure" sur t√¢ches agentic, [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1plbjqg/))

### Devstral-small-2 (Recommand√©)

```bash
# Default
cco
# Or explicit
OLLAMA_MODEL=devstral-small-2 cco
# Or with 64K context Modelfile (recommended)
OLLAMA_MODEL=devstral-64k cco
```

**Avantages**:
- ‚úÖ Meilleur mod√®le agentic pour coding (68% SWE-bench)
- ‚úÖ Format tool-calling Mistral/OpenAI standard ‚Üí compatible Claude Code
- ‚úÖ Pas de probl√®me "stuck on Explore" (contrairement √† Qwen2.5)
- 24B param√®tres ‚Üí ~15GB VRAM

**Usage**: D√©veloppement quotidien offline, code propri√©taire

### IBM Granite4 (Long Context)

```bash
OLLAMA_MODEL=ibm/granite4:small-h cco
# Or alias
cco-granite
```

**Avantages**:
- ‚úÖ Architecture hybride Mamba ‚Üí 70% moins de VRAM pour long contexte
- ‚úÖ 1M tokens contexte natif
- ‚úÖ 9B param√®tres actifs seulement ‚Üí rapide

**Limitations**:
- SWE-bench ~62% (inf√©rieur √† Devstral)

**Usage**: Projets avec beaucoup de fichiers, contexte limit√© RAM

### Configuration contexte 64K (CRITIQUE)

‚ö†Ô∏è **IMPORTANT**: Claude Code envoie ~18K tokens de system prompt + tools. Le contexte par d√©faut (4K) cause:
- Hallucinations
- Comportement "stuck on Explore"
- R√©ponses lentes (2-6 minutes au lieu de 5-15 secondes)

**Solution recommand√©e (Modelfile persistant)**:

```bash
# 1. Cr√©er le Modelfile
mkdir -p ~/.ollama
cat > ~/.ollama/Modelfile.devstral-64k << 'EOF'
FROM devstral-small-2
PARAMETER num_ctx 65536
PARAMETER temperature 0.15
EOF

# 2. Cr√©er le mod√®le
ollama create devstral-64k -f ~/.ollama/Modelfile.devstral-64k

# 3. Utiliser
OLLAMA_MODEL=devstral-64k cco
```

**V√©rifier le contexte effectif**: `ollama ps` (pas `ollama show`)

### Sources

- [Ollama Context Documentation](https://docs.ollama.com/context-length)
- [Taletskiy blog](https://taletskiy.com/blogs/ollama-claude-code/)
- [r/LocalLLaMA benchmarks](https://www.reddit.com/r/LocalLLaMA/comments/1plbjqg/)
- [Devstral HuggingFace](https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512)

## Logs avec mod√®les

Le script log maintenant le mod√®le utilis√©:

```bash
$ cat ~/.claude/claude-switch.log

[2026-01-21 16:30:12] [INFO] Provider: GitHub Copilot (via copilot-api) - Model: claude-opus-4.5
[2026-01-21 16:30:12] [INFO] Session started: mode=copilot:claude-opus-4.5 pid=12345
[2026-01-21 16:32:45] [INFO] Session ended: mode=copilot:claude-opus-4.5 duration=2m33s exit=0

[2026-01-21 16:35:01] [INFO] Provider: GitHub Copilot (via copilot-api) - Model: gpt-4.1
[2026-01-21 16:35:01] [INFO] Session started: mode=copilot:gpt-4.1 pid=12567
```

### Analyser l'usage par mod√®le

```bash
# Sessions par mod√®le
grep "mode=copilot:" ~/.claude/claude-switch.log | cut -d':' -f4 | sort | uniq -c

# Exemple output:
# 12 claude-sonnet-4.5
#  5 claude-opus-4.5
#  3 gpt-4.1
#  2 claude-haiku-4.5
```

## Ajouter vos propres aliases

√âditez `~/.zshrc`:

```bash
# Copilot models
alias ccc-gemini='COPILOT_MODEL=gemini-2.5-pro claude-switch copilot'
alias ccc-gemini3='COPILOT_MODEL=gemini-3-flash-preview claude-switch copilot'
alias ccc-grok='COPILOT_MODEL=grok-code-fast-1 claude-switch copilot'
alias ccc-gpt4='COPILOT_MODEL=gpt-4o-2024-11-20 claude-switch copilot'

# Ollama models (already in install.sh)
alias cco-devstral='OLLAMA_MODEL=devstral-small-2 claude-switch ollama'
alias cco-granite='OLLAMA_MODEL=ibm/granite4:small-h claude-switch ollama'

# Custom 64K Modelfile variant
alias cco-64k='OLLAMA_MODEL=devstral-64k claude-switch ollama'

# Reload
source ~/.zshrc
```

## Status avec mod√®le actuel

```bash
$ ccs
=== Claude Code Provider Status ===

Anthropic API:  ‚úì Reachable
copilot-api:    ‚úì Running (:4141)
Ollama:         ‚úì Running (1 models)

=== Recent Sessions ===
[2026-01-21 16:30:12] [INFO] Session started: mode=copilot:claude-opus-4.5 pid=12345
[2026-01-21 16:32:45] [INFO] Session ended: mode=copilot:claude-opus-4.5 duration=2m33s exit=0
```

## Troubleshooting

### Mod√®le non reconnu

Si vous sp√©cifiez un mod√®le inexistant:

```bash
COPILOT_MODEL=invalid-model ccc
```

copilot-api retournera une erreur. V√©rifiez les mod√®les disponibles:

```bash
# Dans les logs de copilot-api start
copilot-api start

# Vous verrez:
# ‚Ñπ Available models:
# - claude-sonnet-4.5
# - claude-opus-4.5
# ...
```

### Mod√®le par d√©faut

Sans `COPILOT_MODEL`, le script utilise `claude-sonnet-4.5` (meilleur compromis).

Pour changer le d√©faut, √©ditez `~/bin/claude-switch` ligne 100:

```bash
local model="${COPILOT_MODEL:-claude-opus-4.5}"  # Nouveau d√©faut: Opus
```

## Recommandations strat√©giques

### D√©veloppement quotidien
```bash
ccc-sonnet  # ou juste `ccc`
```

### Code critique (PR, prod)
```bash
ccc-opus
```

### Questions rapides
```bash
ccc-haiku
```

### Comparaison/exp√©rimentation
```bash
# Tester plusieurs mod√®les
ccc-sonnet  # Baseline Claude
ccc-gpt     # Alternative GPT
ccc-opus    # Maximum quality
```

### Co√ªt zero via Copilot

Tous ces mod√®les sont **gratuits** avec votre abonnement Copilot Pro+, contrairement √† l'API Anthropic directe:

```bash
# Gratuit (Copilot)
ccc-opus   # Claude Opus via Copilot = $0
ccc-gpt    # GPT-4.1 via Copilot = $0

# Payant (Anthropic Direct)
ccd --model opus  # ~$15/million tokens input
```

## Conclusion

Vous avez maintenant acc√®s √† **15+ mod√®les compatibles** via une seule commande:

```bash
# Claude family
ccc-opus, ccc-sonnet, ccc-haiku

# GPT family
ccc-gpt (+ custom aliases)

# Autres
COPILOT_MODEL=<model> ccc
```

Le tout **gratuitement** avec votre abonnement Copilot Pro+ existant.

---

**Astuce Pro**: Cr√©ez un alias pour votre mod√®le pr√©f√©r√©:

```bash
alias cc='ccc-sonnet'  # Votre go-to command
```

---

## üìö Related Documentation

- [Command Reference](COMMANDS.md) - All available commands
- [Decision Trees](DECISION-TREES.md) - Which model for which task
- [Best Practices](BEST-PRACTICES.md) - Strategic model selection
- [MCP Profiles](MCP-PROFILES.md) - Model compatibility with MCP
- [FAQ](FAQ.md) - Model switching questions

---

**Back to**: [Documentation Index](README.md) | [Main README](../README.md)

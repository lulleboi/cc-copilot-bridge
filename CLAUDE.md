# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

**claude-switch** is a multi-provider wrapper for Claude Code CLI that enables seamless switching between:
- **Anthropic Direct**: Official API, best quality
- **GitHub Copilot**: Free with Copilot Pro+ via copilot-api proxy
- **Ollama Local**: 100% private, offline capable

The project consists of 3 main bash scripts and extensive documentation for optimal usage.

## Core Scripts

### 1. claude-switch (Main Script)
Location: `~/bin/claude-switch` (installed via `install.sh`)

**Key Functions:**
- `_run_direct()`: Anthropic API direct connection
- `_run_copilot()`: GitHub Copilot via copilot-api proxy (localhost:4141)
- `_run_ollama()`: Local Ollama models via localhost:11434
- `_check_port()`: Health checks before launching providers
- `_get_mcp_flags()`: Dynamic MCP profile selection based on model
- `_session_start()`/`_session_end()`: Session logging with durations

**Environment Variables Set:**
```bash
# Copilot mode
ANTHROPIC_BASE_URL="http://localhost:4141"
ANTHROPIC_AUTH_TOKEN="<PLACEHOLDER>"  # copilot-api ignores this value
ANTHROPIC_MODEL="${COPILOT_MODEL:-claude-sonnet-4.5}"
DISABLE_NON_ESSENTIAL_MODEL_CALLS="1"

# Ollama mode
ANTHROPIC_BASE_URL="http://localhost:11434"
ANTHROPIC_AUTH_TOKEN="<PLACEHOLDER>"  # Ollama ignores this value
```

**Model Switching:**
- Default Copilot model: `claude-sonnet-4.5`
- Override via `COPILOT_MODEL` env var (25+ models supported)
- Default Ollama model: `devstral-small-2` (configurable via `OLLAMA_MODEL`)
- Backup Ollama model: `ibm/granite4:small-h` (long context, 70% less VRAM)

### 2. install.sh (Installation)
Auto-installer that:
1. Creates `~/bin/` directory if needed
2. Downloads `claude-switch` script
3. Adds shell aliases to `~/.zshrc` or `~/.bashrc`
4. Creates `~/.claude/` directory for logs

**Aliases Created:**
```bash
ccd='claude-switch direct'
ccc='claude-switch copilot'
cco='claude-switch ollama'
ccs='claude-switch status'
ccc-opus='COPILOT_MODEL=claude-opus-4.5 claude-switch copilot'
ccc-sonnet='COPILOT_MODEL=claude-sonnet-4.5 claude-switch copilot'
ccc-haiku='COPILOT_MODEL=claude-haiku-4.5 claude-switch copilot'
ccc-gpt='COPILOT_MODEL=gpt-4.1 claude-switch copilot'
ccc-gemini='COPILOT_MODEL=gemini-2.5-pro claude-switch copilot'
ccc-gemini3='COPILOT_MODEL=gemini-3-flash-preview claude-switch copilot'
ccc-gemini3-pro='COPILOT_MODEL=gemini-3-pro-preview claude-switch copilot'
cco-devstral='OLLAMA_MODEL=devstral-small-2 claude-switch ollama'
cco-granite='OLLAMA_MODEL=ibm/granite4:small-h claude-switch ollama'
```

### 3. mcp-check.sh (MCP Diagnostics)
Identifies MCP servers with schema validation issues that fail with GPT-4.1 strict validation.

**Known Issues:**
- `grepai`: object schema missing properties (incompatible with GPT-4.1)

**Usage:**
```bash
mcp-check.sh                # Check configured MCP servers
mcp-check.sh --parse-logs   # Scan recent logs for errors
```

## Architecture

### Session Logging
All sessions logged to `~/.claude/claude-switch.log`:
```
[TIMESTAMP] [LEVEL] message
```

**Log Format:**
- Session start: `mode=<provider> pid=<PID> pwd=<directory>`
- Session end: `mode=<provider> duration=<time> exit=<code>`
- Provider info: `Provider: <name> - Model: <model>`

### Provider Health Checks
Before launching, `claude-switch` verifies:
- **Copilot**: Port 4141 responds (via `nc -z`)
- **Ollama**: Port 11434 responds + model exists (`ollama list`)
- **Anthropic**: Uses existing `ANTHROPIC_API_KEY` from environment

### Model Compatibility Matrix

| Provider | Endpoint | Models | MCP Compatibility |
|----------|----------|--------|-------------------|
| Anthropic | Native | Opus/Sonnet/Haiku | 100% (permissive) |
| Copilot-Claude | /chat/completions | claude-*-4.5 | 100% (permissive) |
| Copilot-GPT | /chat/completions | gpt-4.1, gpt-5, gpt-5-mini | ~80% (strict validation) |
| Copilot-Gemini | /chat/completions | gemini-2.5-pro | ~80% (strict validation) |
| Copilot-Gemini3 | /chat/completions | gemini-3-flash-preview, gemini-3-pro-preview | ‚ö†Ô∏è UNTESTED agentic (via unified fork) |
| Copilot-Codex | /responses | gpt-*-codex | ‚úÖ Tested (via unified fork) |
| Ollama | Native | devstral, granite4, qwen3-coder | 100% (permissive) |

**Unified Fork (PR #167 + #170) - EXPERIMENTAL:**

The unified fork combines two features from caozhiyuan/copilot-api branch 'all':
- **PR #167**: Gemini 3 thinking support (`thought_signature`, `reasoning_text`, `reasoning_opaque`)
- **PR #170**: GPT Codex `/responses` endpoint support

**‚ö†Ô∏è Gemini 3 Warning**: PR #167 adds support for "thinking" response fields. This is **NOT** a fix for tool calling format translation. The core issue (Claude ‚Üí OpenAI ‚Üí Gemini format) may still exist. **Testing required before relying on it.**

**Usage:**
```bash
# Terminal 1: Launch unified fork
ccunified  # Uses scripts/launch-unified-fork.sh

# Terminal 2: Use models
ccc-codex         # gpt-5.2-codex ‚úÖ Tested
ccc-codex-mini    # gpt-5.1-codex-mini ‚úÖ Tested
ccc-gemini3       # gemini-3-flash-preview ‚ö†Ô∏è Untested agentic
ccc-gemini3-pro   # gemini-3-pro-preview ‚ö†Ô∏è Untested agentic
```

**Fork Source:** [caozhiyuan/copilot-api branch 'all'](https://github.com/caozhiyuan/copilot-api/tree/all)
**PR Tracking:** [PR #167](https://github.com/ericc-ch/copilot-api/pull/167), [PR #170](https://github.com/ericc-ch/copilot-api/pull/170)

### MCP Profiles System (Advanced)

**Purpose:** GPT-4.1 applies strict JSON Schema validation that rejects some MCP servers with incomplete schemas. System uses dynamic profile generation to exclude problematic servers.

**Directory Structure:**
```
~/.claude/mcp-profiles/
‚îú‚îÄ‚îÄ excludes.yaml           # SOURCE OF TRUTH
‚îú‚îÄ‚îÄ generated/              # Auto-generated
‚îÇ   ‚îú‚îÄ‚îÄ gpt.json           # GPT models config
‚îÇ   ‚îî‚îÄ‚îÄ gemini.json        # Gemini models config
‚îî‚îÄ‚îÄ generate.sh            # Profile generator
```

**Behavior:**
- Claude models ‚Üí Use default `~/.claude/claude_desktop_config.json` (all MCPs)
- GPT models ‚Üí Use `generated/gpt.json` (excludes grepai)
- Gemini models ‚Üí Use `generated/gemini.json` (excludes grepai)

**Regenerate after config changes:**
```bash
~/.claude/mcp-profiles/generate.sh
```

## Performance Considerations

### Ollama Context Size vs Claude Code Requirements

**Critical Issue:** Claude Code sends ~18K tokens of system prompt + tools. Default Ollama context (4K) causes truncation, hallucinations, and slow responses.

**Solution: Create a 64K Modelfile (persistent):**
```bash
mkdir -p ~/.ollama
cat > ~/.ollama/Modelfile.devstral-64k << 'EOF'
FROM devstral-small-2
PARAMETER num_ctx 65536
PARAMETER temperature 0.15
EOF

ollama create devstral-64k -f ~/.ollama/Modelfile.devstral-64k
```

**Verify effective context:** `ollama ps` (not `ollama show`)

**Memory footprint on M4 Pro 48GB with 64K context:**
- Devstral Q4_K_M (24B): 18-22GB model + 12-15GB KV cache = **30-37GB total**
- Granite4 (32B): 22-26GB model + 12-15GB KV cache = **34-41GB total**
- **Minimum RAM**: 32GB for 24B models, **48GB recommended** for 32B + 64K context

**KV Cache Quantization (Ollama 2025 feature):**
- Enable with `OLLAMA_KV_CACHE_TYPE=q4_0` to reduce cache memory by ~75%
- Enables 64K context on 32GB machines (previously required 48GB+)

**Recommendations by Project Size:**
| Project Size | Files | Recommended Solution |
|--------------|-------|---------------------|
| Small | <500 | Ollama with Modelfile 64K ‚ö° |
| Medium | 500-2K | Copilot ‚ö° or Ollama 64K |
| Large | >2K | Copilot/Anthropic ‚ö° |
| Privacy-critical | Any | Ollama 64K (private) üîí |

**Check context usage:** Run `/context` in Claude Code session

### Ollama Models (Updated January 2026)

| Model | Size | SWE-bench | Context | Use Case |
|-------|------|-----------|---------|----------|
| **devstral-small-2** (default) | 24B | 68% | 256K native | Best agentic coding |
| ibm/granite4:small-h | 32B (9B active) | ~62% | 1M | Long context, 70% less VRAM |
| qwen3-coder:30b | 30B | 85% | 256K | Highest accuracy, needs template work |

**‚ö†Ô∏è Models NOT recommended for agentic tasks:**
| Model | SWE-bench | Why Not |
|-------|-----------|---------|
| CodeLlama:13b | ~40% | No tool calling, weak on multi-file editing |
| Llama3.1:8b | **15%** | "Catastrophic failure" on agentic tasks - cannot reliably use tools |

> **Note**: High HumanEval scores (Llama3.1:8b = 68%) do NOT indicate agentic capability. SWE-bench measures real GitHub issue resolution, which requires tool use and multi-step reasoning.

**Sources:**
- [Taletskiy blog](https://taletskiy.com/blogs/ollama-claude-code/)
- [docs.ollama - Context](https://docs.ollama.com/context-length)
- [r/LocalLLaMA benchmarks](https://www.reddit.com/r/LocalLLaMA/comments/1plbjqg/)

> **Note MLX vs GGUF** : Ollama utilise exclusivement GGUF (format universel). Pour performance maximale sur Mac avec petits mod√®les (<22B params), LM Studio + MLX peut √™tre jusqu'√† 4x plus rapide. Cependant, pour mod√®les >30B, GGUF redevient plus performant. LM Studio n'est pas compatible avec claude-switch. [Source: lmstudio-ai/mlx-engine#101](https://github.com/lmstudio-ai/mlx-engine/issues/101)

## Commands for Development

### Testing Providers
```bash
# Check all provider status
ccs

# Test Anthropic Direct
ccd
# Expects: ANTHROPIC_API_KEY set in environment

# Test GitHub Copilot (requires copilot-api running)
copilot-api start  # In separate terminal
ccc

# Test Ollama (requires ollama serve + model pulled)
brew services restart ollama
ollama pull devstral-small-2
# Create 64K Modelfile (see Performance Considerations section)
OLLAMA_MODEL=devstral-64k cco
```

### Debugging Session Issues
```bash
# View recent logs
tail -20 ~/.claude/claude-switch.log

# Check provider health
nc -z localhost 4141  # Copilot
nc -z localhost 11434 # Ollama
curl -s https://api.anthropic.com/v1/messages # Anthropic

# View session durations
grep "Session ended" ~/.claude/claude-switch.log

# Filter by provider
grep "mode=copilot" ~/.claude/claude-switch.log
grep "mode=ollama" ~/.claude/claude-switch.log
```

### Model Switching Commands
```bash
# Copilot with different models
COPILOT_MODEL=claude-opus-4.5 ccc      # Best quality
COPILOT_MODEL=claude-haiku-4.5 ccc     # Fastest
COPILOT_MODEL=gpt-4.1 ccc              # GPT alternative

# Ollama with different models (use 64K Modelfile versions)
OLLAMA_MODEL=devstral-64k cco          # Default (best agentic)
OLLAMA_MODEL=ibm/granite4:small-h cco  # Long context, 70% less VRAM
```

### MCP Troubleshooting
```bash
# Check MCP compatibility
mcp-check.sh

# Scan logs for MCP errors
mcp-check.sh --parse-logs

# Regenerate MCP profiles after config changes
~/.claude/mcp-profiles/generate.sh

# Verify profile content
cat ~/.claude/mcp-profiles/generated/gpt.json | jq -r '.mcpServers | keys[]'
```

## Common Issues & Solutions

### Issue: Ollama Extremely Slow or Hallucinating
**Cause:** Default Ollama context (4K) is too low for Claude Code (~18K system prompt + tools)
**Solution:**
1. **Recommended:** Create a 64K Modelfile (persistent):
   ```bash
   mkdir -p ~/.ollama
   cat > ~/.ollama/Modelfile.devstral-64k << 'EOF'
   FROM devstral-small-2
   PARAMETER num_ctx 65536
   PARAMETER temperature 0.15
   EOF
   ollama create devstral-64k -f ~/.ollama/Modelfile.devstral-64k
   OLLAMA_MODEL=devstral-64k cco
   ```
2. **Alternative:** Quick fix (global, less priority):
   ```bash
   launchctl setenv OLLAMA_CONTEXT_LENGTH 65536
   brew services restart ollama
   ```
3. **Verify:** `ollama ps` should show CONTEXT = 65536

### Issue: "copilot-api not running on :4141"
**Solution:**
```bash
copilot-api start
# Keep running in separate terminal
```

### Issue: Model not found
**Solution:**
```bash
# Pull recommended model
ollama pull devstral-small-2
# Or backup model for long context
ollama pull ibm/granite4:small-h
```

### Issue: MCP Schema Validation Error (GPT-4.1)
**Example:** `Invalid schema for function 'mcp__grepai__grepai_index_status'`
**Solution:**
1. **Preferred:** Use Claude models (`ccc-sonnet`, `ccc-opus`) - 100% MCP compatible
2. **Alternative:** Disable problematic MCP server in `~/.claude/claude_desktop_config.json`
3. **Alternative:** Use MCP profiles system (automatically excludes grepai for GPT)

### Issue: "model gpt-5.2-codex is not accessible via /chat/completions endpoint"
**Cause:** ALL GPT Codex models require `/responses` endpoint (copilot-api v0.7.0 doesn't support it)
**Solution:** Use compatible models:
- `gpt-4.1` (0x premium, recommended)
- `gpt-5` (1x premium)
- `gpt-5-mini` (0x premium, fastest)

## File Organization Rules

### DO NOT modify these files directly:
- `~/.claude/mcp-profiles/generated/*.json` - Auto-generated by `generate.sh`

### Modify these when needed:
- `~/.claude/mcp-profiles/excludes.yaml` - Add problematic MCP servers
- `~/.claude/claude_desktop_config.json` - Base MCP configuration
- `~/bin/claude-switch` - Script modifications

### Documentation structure:
- `README.md` - Complete documentation
- `QUICKSTART.md` - 2-minute setup guide
- `COMMANDS.md` - Command reference
- `TROUBLESHOOTING.md` - Common issues & solutions
- `MODEL-SWITCHING.md` - Dynamic model selection guide
- `MCP-PROFILES.md` - MCP compatibility system
- `OPTIMISATION-M4-PRO.md` - Performance optimization (Apple Silicon)

## Strategic Provider Selection

| Scenario | Command | Reasoning |
|----------|---------|-----------|
| Production code | `ccd` or `ccc-opus` | Best quality, critical decisions |
| Daily development | `ccc` | Free, fast, Claude Sonnet |
| Quick questions | `ccc-haiku` | Fastest responses |
| Code review | `ccc-opus` | Maximum quality |
| Learning/prototyping | `ccc` | Cost-effective iteration |
| Fast iteration | `ccc-gemini` | Gemini 2.5 Pro (stable) |
| Alternative perspective | `ccc-gpt` | GPT-4.1 for second opinion |
| Proprietary code | `cco` | 100% private, no data leaves machine |
| Offline work | `cco` | No internet required |
| Best agentic local | `cco-devstral` | Devstral-small-2 (68% SWE-bench) |
| Long context local | `cco-granite` | Granite4 (70% less VRAM) |

## Package Managers Distribution

### Overview

**Version 1.5.2+** supports distribution via package managers (Homebrew, .deb, .rpm) pour une installation propre et standardis√©e.

**Avantages** :
- ‚úÖ Installation standard (`brew install`, `apt install`, `dnf install`)
- ‚úÖ Gestion automatique des d√©pendances (netcat, etc.)
- ‚úÖ Updates faciles (`brew upgrade`, `apt upgrade`)
- ‚úÖ D√©sinstallation propre
- ‚úÖ Pas d'ex√©cution de scripts curl | bash
- ‚úÖ Configuration shell via `--shell-config` (dynamique)

### Installation Methods

**Homebrew (macOS/Linux)** :
```bash
brew tap FlorianBruniaux/tap
brew install claude-switch
eval "$(claude-switch --shell-config)"
```

**Debian/Ubuntu (.deb)** :
```bash
VERSION="1.5.2"
wget https://github.com/FlorianBruniaux/cc-copilot-bridge/releases/download/v${VERSION}/claude-switch_${VERSION}.deb
sudo dpkg -i claude-switch_${VERSION}.deb
eval "$(claude-switch --shell-config)"
```

**RHEL/Fedora (.rpm)** :
```bash
VERSION="1.5.2"
wget https://github.com/FlorianBruniaux/cc-copilot-bridge/releases/download/v${VERSION}/claude-switch-${VERSION}-1.noarch.rpm
sudo rpm -i claude-switch-${VERSION}-1.noarch.rpm
eval "$(claude-switch --shell-config)"
```

### Shell Configuration

**Option `--shell-config`** (dynamique, recommand√©e) :

```bash
# Test imm√©diat
source <(claude-switch --shell-config)

# Ajouter au .zshrc/.bashrc (toujours √† jour)
eval "$(claude-switch --shell-config)"
```

**Avantages** :
- G√©n√©ration dynamique des aliases
- Toujours √† jour avec le script
- Pas de fichier statique √† maintenir
- Compatible antigen, oh-my-zsh, zinit

**Alternative** (fichier statique) :
```bash
# G√©n√©rer une fois
claude-switch --shell-config > ~/.claude/aliases.sh

# Sourcer dans .zshrc
source ~/.claude/aliases.sh

# Ou avec antigen
antigen bundle ~/.claude/aliases.sh
```

### GitHub Actions Build Pipeline

**D√©clencheur** : Push d'un tag Git (ex: `git tag v1.5.2 && git push --tags`)

**√âtapes automatiques** :
1. Compute SHA256 pour Homebrew Formula
2. Build package `.deb` (Debian/Ubuntu)
3. Build package `.rpm` (RHEL/Fedora/CentOS)
4. Update `Formula/claude-switch.rb` avec SHA256
5. Create GitHub Release
6. Attach packages (.deb, .rpm, .rb) aux assets
7. Commit Formula update dans le repo

**Workflow** : `.github/workflows/build-packages.yml`

### Homebrew Tap Structure

```
FlorianBruniaux/homebrew-tap/
‚îú‚îÄ‚îÄ Formula/
‚îÇ   ‚îî‚îÄ‚îÄ claude-switch.rb    # Homebrew formula
‚îî‚îÄ‚îÄ README.md
```

**Convention** : Le repo `homebrew-tap` devient le tap `FlorianBruniaux/tap`

**Formula** : `Formula/claude-switch.rb` (Ruby)
- URL du tarball source (GitHub archive)
- SHA256 checksum (s√©curit√©)
- D√©pendances (`netcat`, optionnel: `ollama`, `node`)
- Installation script (copie dans `/usr/local/bin`)

### Release Process

**Quick checklist** :
```bash
# 1. Update version
vim claude-switch  # Version: 1.5.3
vim Formula/claude-switch.rb  # version "1.5.3"

# 2. Commit & tag
git add -A
git commit -m "Release v1.5.3: Description"
git tag -a v1.5.3 -m "Release v1.5.3"

# 3. Push tag (triggers GitHub Actions)
git push origin main
git push origin v1.5.3

# 4. Wait for GitHub Actions (~5-10 min)
# Check: https://github.com/FlorianBruniaux/cc-copilot-bridge/actions

# 5. Update Homebrew tap
cd ../homebrew-tap
cp ../cc-copilot-bridge/Formula/claude-switch.rb Formula/
git add Formula/claude-switch.rb
git commit -m "Update claude-switch to v1.5.3"
git push

# 6. Test
brew update
brew upgrade claude-switch
```

**D√©tails complets** : Voir `docs/RELEASE-PROCESS.md`

### Documentation

- **PACKAGE-MANAGERS.md** : Guide utilisateur complet (installation, int√©gration shell)
- **PACKAGE-MANAGERS-EXPLAINED.md** : Explication technique d√©taill√©e (Homebrew, .deb, .rpm, GitHub Actions)
- **RELEASE-PROCESS.md** : Process complet de release (checklist, troubleshooting, rollback)
- **INSTALL-OPTIONS.md** : Int√©gration avec antigen, oh-my-zsh, zinit, etc.

### Testing Locally

**Homebrew Formula** :
```bash
# Build from local formula
brew install --build-from-source Formula/claude-switch.rb

# Verify
claude-switch --version
eval "$(claude-switch --shell-config)"
```

**Debian Package** :
```bash
# Build .deb
dpkg-deb --build deb-build/claude-switch_1.5.2

# Install
sudo dpkg -i claude-switch_1.5.2.deb

# Verify
which claude-switch
```

**RPM Package** :
```bash
# Build .rpm
rpmbuild --define "_topdir $(pwd)/rpm-build" -ba rpm-build/SPECS/claude-switch.spec

# Install (Fedora/RHEL)
sudo rpm -i rpm-build/RPMS/*/claude-switch-*.rpm
```

### Troubleshooting Packages

**Homebrew SHA256 mismatch** :
```bash
# Recalculer SHA256
wget https://github.com/FlorianBruniaux/cc-copilot-bridge/archive/refs/tags/v1.5.2.tar.gz
sha256sum v1.5.2.tar.gz

# Update Formula
sed -i 's/sha256 ".*"/sha256 "NEW_SHA"/' Formula/claude-switch.rb
```

**Debian dependency issues** :
```bash
sudo apt-get install -f  # Fix broken dependencies
```

**RPM build fails** :
```bash
# V√©rifier que le tarball source existe
ls rpm-build/SOURCES/claude-switch-*.tar.gz
```

## Known Issues & Patches

### copilot-api Issue #174: Reserved Billing Header

**Probl√®me**: Claude Code v2.1.15+ injecte `x-anthropic-billing-header` dans le system prompt, causant une erreur `400 invalid_request_body` avec copilot-api.

**Solution 1: Variable d'environnement (Recommand√©e)**

D√©sactive l'injection du header √† la source. Ajouter dans `~/.claude/settings.json`:

```json
{
  "env": {
    "CLAUDE_CODE_ATTRIBUTION_HEADER": "0"
  }
}
```

Ou via variable d'environnement shell:
```bash
export CLAUDE_CODE_ATTRIBUTION_HEADER=0
```

**Solution 2: Patch regex dans copilot-api (Alternative)**

Filtre le header c√¥t√© proxy. Utile si la Solution 1 ne fonctionne pas ou pour une protection d√©fensive.

Fichier: `~/.nvm/versions/node/v22.18.0/lib/node_modules/copilot-api/dist/main.js`

Modification dans `translateAnthropicMessagesToOpenAI`:
```javascript
// FIX #174: Filter x-anthropic-billing-header
systemMessages = systemMessages.map((it) => {
    if (typeof it.content === "string" && it.content.startsWith("x-anthropic-billing-header")) {
        it.content = it.content.replace(/^x-anthropic-billing-header:[^\n]*\n+/, "");
    }
    return it;
});
```

**V√©rification**:
```bash
# Test rapide
ccc -p "1+1"
# Attendu: R√©ponse sans erreur 400
```

**Note**: Le patch regex sera √©cras√© lors de `npm update -g copilot-api`. La Solution 1 est pr√©f√©rable car persistante.

**Suivi**: [ericc-ch/copilot-api#174](https://github.com/ericc-ch/copilot-api/issues/174)

---

### copilot-api Issue #151: Gemini Agentic Mode Limitations

**Probl√®me**: Gemini models (2.5-pro, 3-pro-preview, 3-flash-preview) ont une compatibilit√© limit√©e avec le mode agentic (tool calling, file creation, MCP tools) via copilot-api.

**Sympt√¥mes**:
- ‚úÖ Prompts simples fonctionnent : `COPILOT_MODEL=gemini-3-pro-preview ccc -p "1+1"` ‚Üí OK
- ‚ùå File creation √©choue : `COPILOT_MODEL=gemini-3-pro-preview ccc -p "Create hello.txt"` ‚Üí No file created
- ‚ùå MCP tools instables : `Use grep to find TODOs` ‚Üí Inconsistent results
- ‚ùå Erreurs possibles : `model_not_supported`, `INVALID_ARGUMENT`, `invalid_request_body`

**Cause**: Traduction Claude tool calling ‚Üí OpenAI ‚Üí Gemini format introduit des incompatibilit√©s. Gemini utilise un format tool calling sp√©cifique √† Google qui diff√®re de Claude et OpenAI.

**Models affected**:

| Model | Simple Prompts | Agentic Mode | Status |
|--------|----------------|--------------|--------|
| `gemini-2.5-pro` | ‚úÖ OK | ‚ö†Ô∏è Limited | Deprecating 2/17/26 |
| `gemini-3-pro-preview` | ‚úÖ OK | ‚ö†Ô∏è UNTESTED via fork | Experimental |
| `gemini-3-flash-preview` | ‚úÖ OK | ‚ö†Ô∏è UNTESTED via fork | Experimental |

**Solutions**:

**Option 0: Test Unified Fork (EXPERIMENTAL)** üß™

The unified fork adds PR #167 (Gemini 3 thinking) + PR #170 (Codex).

**‚ö†Ô∏è Important caveat**: PR #167 adds support for Gemini 3 "thinking" response fields (`thought_signature`, `reasoning_opaque`). This is **NOT the same** as fixing tool calling format translation. The core issue (Claude ‚Üí OpenAI ‚Üí Gemini) may still exist.

```bash
# Terminal 1: Launch unified fork
ccunified  # Or ~/Sites/perso/cc-copilot-bridge/scripts/launch-unified-fork.sh

# Terminal 2: Test Gemini 3
ccc-gemini3       # gemini-3-flash-preview
ccc-gemini3-pro   # gemini-3-pro-preview

# Agentic test (uncertain result):
‚ùØ Create test.txt with "hello"
# Check: Was the file created?
```

**Option 1: Use Claude (Recommended - 100% compatible)** ‚≠ê

```bash
ccc-sonnet  # Claude Sonnet 4.5 (default)
ccc-opus    # Claude Opus 4.5 (best quality)
ccc-haiku   # Claude Haiku 4.5 (ultra rapide)

‚ùØ Create hello.txt with "test"
‚úÖ Fonctionne parfaitement
```

**Option 2: Workaround Subagent pour Gemini 3 Preview**

Route les tool calls complexes via GPT-5-mini :

```bash
COPILOT_MODEL=gemini-3-pro-preview CLAUDE_CODE_SUBAGENT_MODEL=gpt-5-mini ccc

‚ùØ Create hello.txt with "test"
‚úÖ Subagent GPT g√®re les tool calls
```

**Option 3: Utiliser Gemini UNIQUEMENT pour prompts simples**

```bash
# ‚úÖ Sc√©narios adapt√©s
COPILOT_MODEL=gemini-2.5-pro ccc -p "Explain this code"
COPILOT_MODEL=gemini-2.5-pro ccc -p "Find bugs"

# ‚ùå Sc√©narios √† √©viter (utiliser Claude √† la place)
# COPILOT_MODEL=gemini-3-pro-preview ccc -p "Create file"  # ‚ùå
ccc-sonnet -p "Create file"  # ‚úÖ
```

**Option 4: Utiliser GPT-4.1 (Alternative stable)**

```bash
COPILOT_MODEL=gpt-4.1 ccc
# Bon compromis entre stabilit√© et compatibilit√©
```

**Workaround automatique dans claude-switch**:

Ajouter dans `~/bin/claude-switch`, fonction `_run_copilot()`:

```bash
# Gemini workaround: auto-set subagent for preview models
if [[ "$COPILOT_MODEL" == gemini-3-*-preview ]]; then
    export CLAUDE_CODE_SUBAGENT_MODEL="${CLAUDE_CODE_SUBAGENT_MODEL:-gpt-5-mini}"
    _log "INFO" "Gemini preview detected: subagent=$CLAUDE_CODE_SUBAGENT_MODEL"
fi
```

**Diagnostic**:

```bash
# Dans le projet cc-copilot-bridge
./scripts/test-gemini.sh

# Voir le rapport
cat debug-gemini/diagnostic-report.md

# Analyser les logs copilot-api
./scripts/analyze-copilot-logs.sh debug-gemini/copilot-api-verbose.log
```

**Tests de compatibilit√©**:

Le script `test-gemini.sh` ex√©cute 5 tests :
1. **Test 1** - Baseline simple (non-agentic) ‚Üí V√©rifie si Gemini fonctionne de base
2. **Test 2** - File creation ‚Üí D√©tecte probl√®me tool calling
3. **Test 3** - MCP grep tool ‚Üí V√©rifie compatibilit√© MCP
4. **Test 4** - Subagent workaround ‚Üí Valide si GPT subagent corrige le probl√®me
5. **Test 5** - Gemini 2.5 stable ‚Üí Compare avec version stable

**Arbre de d√©cision bas√© sur les tests**:
```
Test 1 √©choue ‚Üí Probl√®me auth/config copilot-api (pas Gemini-sp√©cifique)
Test 2 √©choue, Test 1 OK ‚Üí Probl√®me tool format (Gemini-sp√©cifique)
Test 3 √©choue ‚Üí Probl√®me MCP schema validation
Test 4 OK, Test 2 √©choue ‚Üí Workaround subagent fonctionne
Test 5 OK, Test 2 √©choue ‚Üí Gemini 3 preview moins stable que 2.5
```

**Recommandations par sc√©nario**:

| Sc√©nario | Commande Recommand√©e | Raison |
|----------|---------------------|--------|
| Production code | `ccc-sonnet` | 100% fiable, meilleure qualit√© |
| Questions rapides | `ccc-haiku` | Rapide, fiable, pas de complexit√© Gemini |
| Code review | `ccc-opus` | Qualit√© maximale |
| Exp√©rimentation Gemini | `COPILOT_MODEL=gemini-3-pro-preview CLAUDE_CODE_SUBAGENT_MODEL=gpt-5-mini ccc` | Workaround subagent |
| Alternative GPT | `COPILOT_MODEL=gpt-4.1 ccc` | Stable, bon compromis |

**Migration Path**:

```
Actuellement Gemini 2.5 Pro:
‚îú‚îÄ Prompts simples ‚Üí Continue avec gemini-2.5-pro
‚îú‚îÄ Agentic tasks ‚Üí Migre vers ccc-sonnet
‚îî‚îÄ Apr√®s 17 f√©v 2026 ‚Üí Tout vers ccc-sonnet

Actuellement Gemini 3 Preview:
‚îî‚îÄ Tout ‚Üí Migre vers ccc-sonnet (plus stable, meilleure qualit√©)
```

**Documentation compl√®te**:
- [docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md#-gemini-agentic-mode-issues-copilot-api) - Section Gemini Agentic Mode Issues
- [docs/MODEL-SWITCHING.md](docs/MODEL-SWITCHING.md#mod√®les-gemini-via-copilot) - Tableau de compatibilit√© Gemini
- [debug-gemini/README.md](debug-gemini/README.md) - Testing workspace
- [scripts/test-gemini.sh](scripts/test-gemini.sh) - Automated diagnostic suite

**Suivi**: [ericc-ch/copilot-api#151](https://github.com/ericc-ch/copilot-api/issues/151)

---

## Version Information

- **claude-switch**: v1.5.1 (2026-01-23) - Added unified fork (Codex tested, Gemini 3 experimental)
- **copilot-api**: v0.7.0 (official) + unified fork (PR #167 + #170)
  - Official: `/chat/completions` only - voir issue #174 pour fix billing header
  - Unified fork: Gemini 3 thinking support + Codex `/responses` endpoint
  - ‚ö†Ô∏è Gemini 3 agentic mode: UNTESTED - PR #167 adds thinking, not tool calling fix
  - Fork source: [caozhiyuan/copilot-api branch 'all'](https://github.com/caozhiyuan/copilot-api/tree/all)
- **Claude Code CLI**: v2.1.15 (@anthropic-ai/claude-code npm package)
- **Ollama**: Homebrew service, default model: devstral-small-2 (backup: ibm/granite4:small-h)

## Testing Changes

When modifying `claude-switch`:
1. Test with all 3 providers (`ccd`, `ccc`, `cco`)
2. Check session logs: `tail ~/.claude/claude-switch.log`
3. Verify health checks: `ccs`
4. Test model switching: `COPILOT_MODEL=<model> ccc`
5. Test error handling: Stop provider and try launching

## Notes for AI Assistants

- All bash scripts use `set -euo pipefail` for safety
- Port conflicts: Copilot (4141), Ollama (11434)
- Session tracking via log file enables usage analytics
- MCP profiles prevent runtime errors with strict validation models
- Default models chosen for best quality/speed balance
- Logs are append-only, consider rotation for long-term use
